{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8942a269-6154-4de7-89ad-76d77a26fe3b",
   "metadata": {},
   "source": [
    "a_tensor_initialization.py\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9037313b-4c01-4fbd-bf2e-d7389cb1d02a",
   "metadata": {},
   "source": [
    "### 텐서 초기화\n",
    "1. torch.Tensor 클래스: PyTorch의 기본 텐서 타입 클래스. torch.FloatTensor의 별칭이며 이름대로 텐서를 float32 데이터 타입으로 생성한다. 이 데이터 타입은 고정이므로 클래스를 사용할 때 dtype 인자를 사용할 수 없다. 현재는 잘 사용하지 않는 생성 방식.\n",
    "* 예: torch.Tensor([1, 2, 3])으로 만들고 출력하면 [1., 2., 3.]으로 나온다.\n",
    "  \n",
    "2. torch.tensor 함수: 따로 dtype 인자를 사용하지 않으면 입력된 데이터의 타입으로 자동으로 설정된다.\n",
    "* 예: torch.tensor([1, 2, 3], dtype=torch.int64), torch.Tensor([1, 2, 3]).to(torch.int64), torch.LongTensor([1, 2, 3]). 모두 [1, 2, 3]으로 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa4f3f86-b84b-43df-93bc-70ca12a9f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af81761-e399-4c83-abae-05b5590da9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.Tensor([1, 2, 3], device='cpu')\n",
    "print(t1.dtype)   # >>> torch.float32\n",
    "print(t1.device)  # >>> cpu\n",
    "print(t1.requires_grad)  # >>> False\n",
    "print(t1.size())  # torch.Size([3])\n",
    "print(t1.shape)   # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t1_cuda = t1.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t1_cuda = t1.cuda()\n",
    "t1_cpu = t1.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4605b1-fd6b-468b-b2ed-6f1967c72a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor function: 모든 텐서는 device, dtype, shape, requires_grad 속성이 있다.\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(t2.dtype)  # 텐서의 데이터 타입 (float32, int64 등)\n",
    "print(t2.device)  # 텐서가 저장된 장치 (CPU, GPU 등)\n",
    "print(t2.requires_grad)  # 텐서의 자동 미분 허용 여부. 텐서는 기본값 None인 기울기 속성을 가지고 있고 역전파 과정을 통해 기울기가 계산된다.\n",
    "                         # requires_grad를 True로 설정해야 역전파 과정해서 계산된 기울기값이 텐서의 기울기 속성에 할당된다.\n",
    "print(t2.size())\n",
    "print(t2.shape)  # 텐서의 크기=차원 구조\n",
    "\n",
    "# if you have gpu device\n",
    "# t2_cuda = t2.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t2_cuda = t2.cuda()\n",
    "t2_cpu = t2.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e5fc3a-5569-490c-b99a-3f8b678de654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     57\u001b[39m a10 = torch.tensor([                 \u001b[38;5;66;03m# shape: torch.Size([4, 1, 5]), ndims(=rank): 3\u001b[39;00m\n\u001b[32m     58\u001b[39m     [[\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m]],\n\u001b[32m     59\u001b[39m     [[\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m]],\n\u001b[32m     60\u001b[39m     [[\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m]],\n\u001b[32m     61\u001b[39m     [[\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m]],\n\u001b[32m     62\u001b[39m ])\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(a10.shape, a10.ndim)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m a11 = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# PyTorch 텐서는 모든 차원에서 같은 길이를 가져야한다.\u001b[39;49;00m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# a11은 4×1×2×(마지막 차원) 구조인데, 마지막 차원중 하나는 길이가 3, 다른 하나는 길이가 2라서 서로 다르다.\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "source": [
    "a1 = torch.tensor(1)\t\t\t     # shape: torch.Size([]), ndims(=rank): 0\n",
    "print(a1.shape, a1.ndim)\n",
    "\n",
    "a2 = torch.tensor([1])\t\t  \t     # shape: torch.Size([1]), ndims(=rank): 1\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])   # shape: torch.Size([5]), ndims(=rank): 1\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
    "print(a4.shape, a4.ndim)\n",
    "\n",
    "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "\n",
    "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a11 = torch.tensor([                 # PyTorch 텐서는 모든 차원에서 같은 길이를 가져야한다.\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])\n",
    "# a11은 4×1×2×(마지막 차원) 구조인데, 마지막 차원중 하나는 길이가 3, 다른 하나는 길이가 2라서 서로 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207fbb5f-8d84-4b44-886e-abd829fbe410",
   "metadata": {},
   "source": [
    "b_tensor_initializatioin_copy.py\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67983f28-42f9-4d42-9ad7-86a58468f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# 데이터로부터 직접적으로 텐서 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ac2fbd3-62fe-4cca-a51a-2734fdaf871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor와 torch.tensor는 언제나 주어진 데이터를 복사해서 텐서를 만든다.\n",
    "# torch.Tensor: 주어진 데이터를 float32로 복사해서 텐서를 만든다('.'이 붙는 이유는 그 때문).\n",
    "# 셋 모두 리스트와 numpy에 사용할 수 있다.\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)\n",
    "\n",
    "# torch.tensor: 주어진 데이터를 그대로 복사해서 텐서를 만든다.\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)\n",
    "\n",
    "# torch.as_tensor: 주어진 데이터를 '복사하지 않고' 그대로 참조한다(원본 데이터가 바뀌면 텐서도 같이 바뀜).\n",
    "# 다만 단순 리스트는 참조가 되지 않는다. l3[0]=100이 적용되지 않는 이유도 그 때문.\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96a47360-d6ac-436e-a628-810d841cd618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([100,   2,   3])\n"
     ]
    }
   ],
   "source": [
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6) # torch.as_tensor가 numpy 배열인 l6를 복제하지 않고 그대로 참조하기 때문에 l6[0]=100이 적용된다.\n",
    "          # numpy 배열이 복제되길 원치 않는다면 torch.as_tensor()를 사용하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a813809-d25b-4ad7-b34b-33a3c5ea6ad9",
   "metadata": {},
   "source": [
    "c_tensor_initializatioin_constant_values.py\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b13f36-43b3-4975-8d1d-eca985ca474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 상수로 텐서 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9cbb068-b91c-4d67-a681-13fb5b53261c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# torch.ones(*size): 1로 채워진 *size 크기의 텐서 생성\n",
    "t1 = torch.ones(size=(5,))  # or torch.ones(5)\n",
    "\n",
    "# torch.ones_like(input_tensor): input_tensor와 같은 shape를 가진 1로 채운 텐서 생성\n",
    "t1_like = torch.ones_like(input=t1)\n",
    "\n",
    "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "\n",
    "# 추가 코드\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "tx = torch.ones_like(input=x)\n",
    "print(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eacf2baf-7ee6-43da-ada9-118dfbb1ef8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# torch.zeros(*size): 0으로 채워진 *size 크기의 텐서 생성\n",
    "t2 = torch.zeros(size=(6,))  # or torch.zeros(6)\n",
    "\n",
    "# torch.zeros_like(input_tensor): input_tensor와 같은 shape를 가진 0으로 채운 텐서 생성\n",
    "t2_like = torch.zeros_like(input=t2)\n",
    "\n",
    "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "# 추가 코드\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "tx = torch.zeros_like(input=x)\n",
    "print(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fdfe258-1288-4f2b-b10a-5baec907b844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1698e-19, 7.2587e-43, 3.0000e+00, 0.0000e+00])\n",
      "tensor([0.0000, 4.4766, 0.0000, 0.0000])\n",
      "tensor([[2225338451824,             0,             0],\n",
      "        [            0,             0,             0]])\n"
     ]
    }
   ],
   "source": [
    "# torch.empty(*size): '초기화되지 않은 데이터(무작위값)'로 채워진 *size 크기의 텐서 생성\n",
    "t3 = torch.empty(size=(4,))  # or torch.zeros(4)\n",
    "\n",
    "# torch.empty_like(input_tensor): input_tensor와 같은 shape를 가진 초기화되지 않은 데이터로 채워진 텐서 생성.\n",
    "t3_like = torch.empty_like(input=t3)\n",
    "print(t3)  # >>> tensor([0., 0., 0., 0.])\n",
    "print(t3_like)  # >>> tensor([0., 0., 0., 0.])\n",
    "\n",
    "# 추가 코드\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "tx = torch.empty_like(input=x)\n",
    "print(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6d0d94e-bc5b-45f3-a29d-964c9b1b789a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.eye(n): 단위행렬(대각선만 1이고 나머지는 0인 n×n 텐서) 생성\n",
    "t4 = torch.eye(n=3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b94047-09eb-4e39-8487-07ae3f2e8678",
   "metadata": {},
   "source": [
    "d_tensor_initialization_random_values.py\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37a45888-cd10-4cc9-9ba1-60245eb1079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 무작위 값으로 텐서 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12fedd01-eded-40c6-8731-b810c158b6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 16]])\n"
     ]
    }
   ],
   "source": [
    "# torch.randint(low=0, high, size, ...): low이상 high미만으로 균등하게 분포된 정수 난수로 텐서를 채움.\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8beb03ff-de83-4d00-a1bc-0e1ae0b25e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2013, 0.4258, 0.1916]])\n"
     ]
    }
   ],
   "source": [
    "# torch.rand(*size, ...): 0이상 1미만으로 균등하게 분포된 난수로 텐서를 채움.\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8e6c6e7-b916-4ccb-94e5-331bc86f0eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3902,  0.5602,  0.3644]])\n"
     ]
    }
   ],
   "source": [
    "# torch.randn(*size, ...): 평균이 0이고 분산이 1인 표준정규분포에서 뽑은 float 난수로 텐서를 채움.\n",
    "t3 = torch.randn(size=(1, 3))\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9618293-6065-4fff-ae0c-cdc20d4633ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10.6666,  8.8450],\n",
      "        [ 8.0792, 10.6487],\n",
      "        [11.8867,  9.9282]])\n"
     ]
    }
   ],
   "source": [
    "# torch.normal(mean, std, size, ...): 평균(mean)과 표준편차(std)가 주어진 정규분포에서 뽑은 float 난수로 텐서를 채움.\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81b3a9d4-3b57-45df-8e03-43f791d65503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# torch.linspace(start, end, steps, …): start부터 end까지 균등 간격으로 나눈 steps개의 값들로 채워진 1차원 텐서를 반환.\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
    "print(t5)\n",
    "\n",
    "# 추가 코드\n",
    "t5x = torch.linspace(start=0, end=1, steps=5)\n",
    "print(t5x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f8c31cd-7aef-4f1a-84be-28ae89521df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([ 1,  4,  7, 10, 13])\n"
     ]
    }
   ],
   "source": [
    "# torch.arange(start=0, end, steps=1,…): start이상 end 미만까지 steps 간격으로 생성된 값들로 채워진 1차원 텐서를 반환.\n",
    "t6 = torch.arange(5)\n",
    "print(t6)\n",
    "\n",
    "# 추가 코드\n",
    "t6x = torch.arange(1, 14, 3)\n",
    "print(t6x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab36406-e5ac-40cd-a0b3-2b61b874070b",
   "metadata": {},
   "source": [
    "#### Random seed(무작위 seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "529d21c5-a925-4cc0-ab2c-0af416d72193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(seed): 난수 생성기의 seed를 고정된 값으로 설정함으로써 사용자가 torch.rand(2) 예제를 호출했을 때, 결과가 재현가능해진다. \n",
    "torch.manual_seed(1729) # seed를 1729로 고정\n",
    "random1 = torch.rand(2, 3) # 2×3 shape의 첫 번째 무작위 텐서 생성\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3) # 두 번째 무작위 텐서 생성\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729) # seed를 위와 똑같이 맞춰주면\n",
    "random3 = torch.rand(2, 3) # random3는 random1과\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3) # random4는 random2와\n",
    "print(random4) # 똑같은 결과를 출력한다.\n",
    "# 실험 과정에서 무작위성 때문에 결과가 매번 달라지지 않도록, 한 번 얻은 랜덤 값을 계속 재현해 쓰는 데 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c358e1-32fc-496d-b172-f9407ed04d56",
   "metadata": {},
   "source": [
    "e_tensor_type_conversion.py\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa4864eb-9aad-47ca-931a-d348326316d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 텐서 타입 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cd43807-5197-4582-aa05-2467d93c24ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[18.0429,  7.2532, 19.6519],\n",
      "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n",
      "tensor([[18,  7, 19],\n",
      "        [10,  2, 19]], dtype=torch.int32)\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3))\n",
    "print(a.dtype)\n",
    "\n",
    "b = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(b)\n",
    "\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20. # 생성된 2×3 텐서의 모든 원소 값에 20을 곱함.\n",
    "print(c)\n",
    "\n",
    "d = c.to(torch.int32) # c의 float64값을 int32로 변환=소수점 이하를 버림.\n",
    "print(d)\n",
    "\n",
    "# double은 float64, short는 int64의 약칭.\n",
    "# 아래 4가지 표현(double/short, double()/short(), to(torch.double)/to(torch.short), type(torch.double)/type(torch.short))은 모두 같은 결과임.\n",
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "# 추가 코드\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "short_e = torch.ones(10, 2).short()\n",
    "# 추가 코드\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_e = torch.ones(10, 2).to(torch.short)\n",
    "# 추가 코드\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "\n",
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "short_e = torch.ones(10, 2). type(torch.short)\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "\n",
    "double_f = torch.rand(5, dtype=torch.double)\n",
    "short_g = double_f.to(torch.short)\n",
    "print((double_f * short_g).dtype) # float64인 double_f와 int16인 short_g의 곱셈은 int16을 float64로 변환하여 수행.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a828388-c624-4079-9a80-dbca68c8ea94",
   "metadata": {},
   "source": [
    "f_tensor_operations.py\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "209b406f-cbf8-4f03-80b3-e514304958ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ed803de-65f3-4496-90af-e36c8e090885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "\n",
    "t3 = torch.add(t1, t2) # t1 + t2\n",
    "t4 = t1 + t2\n",
    "\n",
    "print(t3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a488ebb-44dd-4bbd-993f-6faab9afd975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[-5, -3, -1],\n",
      "        [ 1,  3,  5]])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.sub(t1, t2) # t1 - t2\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "# 추가 코드\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "y = torch.tensor([[6, 5, 4], [3, 2, 1]])\n",
    "print(torch.sub(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "648e9e86-30ce-426d-8b43-871541efebd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[ 6, 10, 12],\n",
      "        [12, 10,  6]])\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.mul(t1, t2) # t1 * t2\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "# 추가 코드\n",
    "print(torch.mul(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46c7a5a0-c90b-40ea-9d5a-2e20c61f53c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.1667, 0.4000, 0.7500],\n",
      "        [1.3333, 2.5000, 6.0000]])\n"
     ]
    }
   ],
   "source": [
    "t9 = torch.div(t1, t2) # t1 / t2\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)\n",
    "\n",
    "# 추가 코드\n",
    "print(torch.div(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d89e34-b795-4015-9ab7-18923620c46a",
   "metadata": {},
   "source": [
    "g_tensor_operations_mm.py\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "031b17f0-8598-45d9-a2b0-04dd7be7fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# dot, mm, bmm 모두 broadcasting 지원 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf80fec0-a6a0-41c6-9bd5-f09fad00b147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "tensor([[1.6750, 2.2840],\n",
      "        [0.0956, 1.0294]]) torch.Size([2, 2])\n",
      "torch.Size([10, 3, 5])\n",
      "tensor([[ 5, 12],\n",
      "        [21, 32]]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# torch.dot(input, other): 2개의 1차원 텐서의 내적 연산 수행. 두 텐서의 길이는 같아야 함.\n",
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size())\n",
    "\n",
    "# torch.mm(input, other): broadcasting 없이 2차원 행렬 곱셈 연산 수행.\n",
    "# (n×m)×(m×p)→(n×p)\n",
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3)\n",
    "print(t4, t4.size())\n",
    "\n",
    "# torch.bmm(input, other): broadcasting 없이 배치 행렬 곱셈 연산 수행.\n",
    "# (b×n×m)×(b×m×p)→(b×n×p)\n",
    "# batch는 한 번에 처리하는 데이터 묶음을 의미하며, batch 차원은 행렬의 첫 번째 차원을 의미함.\n",
    "t5 = torch.randn(10, 3, 4) # batch=10, 3×4\n",
    "t6 = torch.randn(10, 4, 5) # batch=10, 4×5\n",
    "t7 = torch.bmm(t5, t6)\n",
    "print(t7.size())\n",
    "\n",
    "# 추가 코드\n",
    "# torch.mul(input, other): broadcasting을 지원하는 원소별 곱셈 연산\n",
    "t8 = torch.mul(\n",
    "    torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]])\n",
    ")\n",
    "print(t8, t8.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4db89e-bda1-466d-9b01-404c68d13496",
   "metadata": {},
   "source": [
    "h_tensor_operations_matmul.py\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74a7a9db-1dcf-4fc7-a938-5d5316695b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8dcbaa2-adb1-4503-9643-e5b9a6b2f320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# torch.matmul(Tensor, Tensor): broadcasting을 지원하는 다양한 텐서 곱 연산.\n",
    "# 입력 텐서의 shape에 따라 다른 모드를 사용한다.\n",
    "# 내적, 행렬, 배치 행렬 곱셈 연산 모두 지원\n",
    "\n",
    "# vector x vector: dot product\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())  # torch.Size([])\n",
    "\n",
    "# matrix x vector: broadcasted dot\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4) # broadcasting으로 4짜리 행렬이 4×1 행렬로 전환되어 연산 실행.\n",
    "print(torch.matmul(t3, t4).size())  # torch.Size([3])\n",
    "\n",
    "# batched matrix x vector: broadcasted dot\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4) # braodcasting으로 4짜리 행렬이 10×4×1 행렬로 전환되어 연산 실행.\n",
    "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])\n",
    "\n",
    "# batched matrix x batched matrix: bmm\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])\n",
    "\n",
    "# batched matrix x matrix: bmm\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5) # broadcasting으로 4×5 행렬이 10×4×5 행렬로 전환되어 연산 실행.\n",
    "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d461e94-2de4-489d-9228-2a150b393d66",
   "metadata": {},
   "source": [
    "i_tensor_broadcasting.py\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65b3c254-15d3-4e8b-9fe0-8927a6baf7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# broadcasting: 텐서 간 연산 시 작은 텐서가 큰 텐서와 호환되는 shape이 되도록 broadcast되어 두 텐서 간 원소별 연산이 수행될 수 있도록 해준다.\n",
    "# broadcasting 시 더 작은 배열이 더 더 큰 배열에 맞게 새로운 축이 추가되고, 변환된 배열에 맞게 데이터가 적절히 더해진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "598f3171-c00f-4f32-8af4-0b643a144d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0 # 스칼라 텐서가 [2.0, 2.0, 2.0]로 변환됨.\n",
    "print(t1 * t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "404732c5-1977-4107-9bc1-fa4a1874aa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5]) # shape이 2인 행렬이 broadcasting으로 3×2로 변환됨.\n",
    "                          # ([4, 5], [4, 5], [4, 5])\n",
    "print(t3 - t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50a5eb6f-44d5-460d-8395-d02b2c43060b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)  # t5.add(2.0)\n",
    "print(t5 - 2.0)  # t5.sub(2.0)\n",
    "print(t5 * 2.0)  # t5.mul(2.0)\n",
    "print(t5 / 2.0)  # t5.div(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a13e8a47-2f4a-40d4-a21d-9e9025602cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "tensor([[[-0.3970,  0.7016,  0.9221,  ..., -0.1537,  0.7817,  0.7875],\n",
      "         [ 1.8459, -2.5061,  0.6333,  ...,  2.1567,  0.5248,  0.7586],\n",
      "         [ 0.1338, -0.6398, -0.3082,  ..., -0.3877, -2.3846,  1.2006],\n",
      "         ...,\n",
      "         [ 1.2316,  0.9079,  0.0856,  ...,  1.2696, -0.4395,  0.4958],\n",
      "         [-0.1648,  0.4693, -0.6671,  ..., -0.2604, -0.9668,  0.4738],\n",
      "         [-0.2336, -1.1732,  1.6757,  ...,  0.0724,  0.4495,  1.7352]],\n",
      "\n",
      "        [[ 1.2012,  0.9665,  3.2020,  ..., -0.5740,  0.0999, -1.3696],\n",
      "         [-0.3583,  0.8137,  0.1895,  ...,  0.5906,  0.7782, -1.1740],\n",
      "         [ 0.0834,  1.0572, -0.0782,  ...,  0.2611, -0.9627, -0.3044],\n",
      "         ...,\n",
      "         [-0.9768,  0.6411, -0.7873,  ..., -0.7498,  0.6062, -1.1202],\n",
      "         [ 1.1020,  0.9198, -0.3662,  ...,  1.2218,  0.1075, -0.9857],\n",
      "         [ 1.7829, -0.2359, -0.3692,  ..., -0.6503,  1.3735, -0.9018]],\n",
      "\n",
      "        [[ 0.6798, -0.0925,  0.9455,  ...,  0.1497,  0.5510, -0.8752],\n",
      "         [-0.3503, -0.2982, -0.6690,  ..., -0.1518, -1.4337, -0.3300],\n",
      "         [-1.4378,  1.8931, -0.3457,  ..., -0.0620, -0.2615,  0.0341],\n",
      "         ...,\n",
      "         [-0.7720,  0.2669, -1.5662,  ...,  0.7656,  1.0310,  0.4047],\n",
      "         [ 1.5665, -0.1920, -0.2951,  ...,  0.8249,  0.5199, -2.2862],\n",
      "         [ 0.3495,  0.7140,  0.4677,  ..., -0.6843,  0.8586,  0.4501]]])\n",
      "tensor([[[-0.0016,  0.0028,  0.0036,  ..., -0.0006,  0.0031,  0.0031],\n",
      "         [ 0.0072, -0.0098,  0.0025,  ...,  0.0085,  0.0021,  0.0030],\n",
      "         [ 0.0005, -0.0025, -0.0012,  ..., -0.0015, -0.0094,  0.0047],\n",
      "         ...,\n",
      "         [ 0.0048,  0.0036,  0.0003,  ...,  0.0050, -0.0017,  0.0019],\n",
      "         [-0.0006,  0.0018, -0.0026,  ..., -0.0010, -0.0038,  0.0019],\n",
      "         [-0.0009, -0.0046,  0.0066,  ...,  0.0003,  0.0018,  0.0068]],\n",
      "\n",
      "        [[ 0.0047,  0.0038,  0.0126,  ..., -0.0023,  0.0004, -0.0054],\n",
      "         [-0.0014,  0.0032,  0.0007,  ...,  0.0023,  0.0031, -0.0046],\n",
      "         [ 0.0003,  0.0041, -0.0003,  ...,  0.0010, -0.0038, -0.0012],\n",
      "         ...,\n",
      "         [-0.0038,  0.0025, -0.0031,  ..., -0.0029,  0.0024, -0.0044],\n",
      "         [ 0.0043,  0.0036, -0.0014,  ...,  0.0048,  0.0004, -0.0039],\n",
      "         [ 0.0070, -0.0009, -0.0014,  ..., -0.0026,  0.0054, -0.0035]],\n",
      "\n",
      "        [[ 0.0027, -0.0004,  0.0037,  ...,  0.0006,  0.0022, -0.0034],\n",
      "         [-0.0014, -0.0012, -0.0026,  ..., -0.0006, -0.0056, -0.0013],\n",
      "         [-0.0056,  0.0074, -0.0014,  ..., -0.0002, -0.0010,  0.0001],\n",
      "         ...,\n",
      "         [-0.0030,  0.0010, -0.0061,  ...,  0.0030,  0.0040,  0.0016],\n",
      "         [ 0.0061, -0.0008, -0.0012,  ...,  0.0032,  0.0020, -0.0090],\n",
      "         [ 0.0014,  0.0028,  0.0018,  ..., -0.0027,  0.0034,  0.0018]]])\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "  return x / 255\n",
    "\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size()) # 텐서의 값에만 적용되는 함수는 텐서의 shape에는 영향 없음.\n",
    "\n",
    "# 추가 코드\n",
    "print(t6)\n",
    "print(normalize(t6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256e4012-0d42-498c-9ea7-1600a7772ca1",
   "metadata": {},
   "source": [
    "### ※ 브로드캐스팅 규칙\n",
    "각 차원은 뒤에서부터 앞으로 가면서 비교한다.\n",
    "1. 각 차원은 같거나\n",
    "2. 각 차원 중 하나는 1이어야 한다.\n",
    "3. 두 텐서 중 하나는 차원이 존재하지 않아야 한다.\n",
    "---------------------------------------\n",
    "#### 예시 문제\n",
    "두 텐서 a=torch.tensor(5, 1, 3, 1), b=torch.tensor(3, 3, 1)이 있다. a+b 연산을 하고자 할 때\n",
    "    \n",
    "    ① 우측 정렬을 시켜준 후 뒤에서부터 앞으로 비교한다.\n",
    "    first←last\n",
    "    5 1 3 1\n",
    "      3 3 1\n",
    "    두 텐서를 비교했을 때 3번 차원(1과 1)은 같으므로 1번 규칙을 충족한다.\n",
    "    다만 0번(5,  )과 1번(1, 3) 차원은 서로 다르다.\n",
    "    이 때 두번째 규칙으로 넘어간다.\n",
    "    ② 1번 차원(1과 3) 중 a가 1이므로 2번 규칙을 만족한다.\n",
    "    ③ 0번 차원(5와  ) 중 b쪽이 차원이 존재하지 않으므로 3번 규칙을 만족한다.\n",
    "    조건을 만족하므로 a와 b는 브로드캐스팅이 시행된다.\n",
    "\n",
    "이렇게 브로드캐스팅이 지원되면서 a+b 연산이 시행되면, 결과값의 *shape*은 각 축을 비교해서 더 큰 쪽으로 나오게 된다.\n",
    "a+b=(5, 1, 3, 1)+(3, 3, 1)=c=(5, 3, 3, 1)\n",
    "=> c의 shape은 (5, 3, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68153389-2b95-4460-8750-4224419fb47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
    "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
    "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
    "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "317c8ee5-76d5-4e5e-9ffe-701f0521b8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
    "print((t17 + t18).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b33e17f7-3d43-489a-9add-9aa78d1aafb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1\n",
    "# a: 5 2 4 1\n",
    "# b:   3 1 1\n",
    "# 0번 차원: 5와    => 3번 규칙 만족\n",
    "# 1번 차원: 2와 3 => 값이 같거나 어느 한쪽이 1도 아니고 둘 다 차원이 존재하므로 1, 2, 3번 규칙 모두 불만족\n",
    "# 2번 차원: 4와 1 => b의 차원이 1이므로 2번 규칙 만족\n",
    "# 3번 차원: 1과 1 => 값이 같으므로 1번 규칙 만족\n",
    "# => 1번 차원이 규칙을 모두 불만족하므로 브로드캐스팅 불가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90262c05-5ad8-4f72-b206-d2fd2e52d0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n",
      "tensor([   3.,   16.,  125., 1296.])\n"
     ]
    }
   ],
   "source": [
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # >>> tensor([ 5, 5, 5, 5])\n",
    "\n",
    "# pow(x, n): 행렬 x의 각 원소를 n만큼 제곱\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)  # >>> tensor([ 25, 25, 25, 25])\n",
    "\n",
    "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "t29 = torch.pow(a, exp) # 각 원소별로 제곱(1^1, 2^2, 3^3, 4^4)\n",
    "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])\n",
    "\n",
    "# 추가 코드\n",
    "exp2 = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "a2 = torch.arange(3., 7.)  # tensor([ 3.,  4.,  5.,  6.])\n",
    "t29 = torch.pow(a2, exp2) # 각 원소별로 제곱(1^3, 2^4, 3^5, 4^6)\n",
    "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d126580-22e2-4b0b-8865-91c438c53d18",
   "metadata": {},
   "source": [
    "j_tensor_indexing_slicing.py\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f2eaa9-faa4-44fb-9e62-3543eba7a6fe",
   "metadata": {},
   "source": [
    "### 텐서 인덱싱 & 슬라이싱\n",
    "- 텐서에서 특정 원소나 구간(슬라이스)을 접근하거나 가져오는 것\n",
    "- 일반 기법\n",
    "  * 기본 인덱싱: 각 차원에 대한 인덱스를 지정하여 텐서의 개별 원소에 접근\n",
    "  * 슬라이싱: 각 차원에서 구간을 지정하여 그 구간만큼 부분 텐서를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68fc50aa-5a46-40de-b92c-cc025717dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b45ede4-521b-464e-8562-304470d0da8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1])  # 1번 행 전체\n",
    "\n",
    "print(x[:, 1])  # 모든 행의 1번 열\n",
    "print(x[1, 2])  # 1행 2열의 원소\n",
    "print(x[:, -1])  # 모든 행의 마지막 열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c61cb30-829f-4c92-b35f-10b17fee2b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n"
     ]
    }
   ],
   "source": [
    "print(x[1:])  # 1행부터 끝까지\n",
    "print(x[1:, 3:])  # 1행부터 마지막 행 중에서 마지막과 그 직전 열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71bca705-5c9f-4c0c-9366-7bed272e85cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.zeros((6, 6)) # 원소가 모두 0인 6×6 텐서를 만들고\n",
    "y[1:4, 2] = 1 # 1번 행부터 3번 행까지 중 2열에 1을 삽입\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4]) # 1~3번행, 1~3번열의 부분 텐서 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3607cf03-9c1d-4170-a42b-e38e092d0cc3",
   "metadata": {},
   "source": [
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2]) # 0~1번 행\n",
    "print(z[1:, 1:3]) # 1행부터 끝행까지 1~2번열 출력\n",
    "print(z[:, 1:]) # 모든 행의 1열부터 끝번열 출력\n",
    "\n",
    "z[1:, 1:3] = 0 # 1행부터 끝행까지 1~2번열에 0 삽입\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24b080da-56ca-4a5f-beb8-794aa41f6e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 7, 8, 9])\n",
      "tensor([10, 30])\n"
     ]
    }
   ],
   "source": [
    "# 추가 코드\n",
    "# 고급 인덱싱: 몇몇 라이브러리들은 boolean mask나 인덱싱 배열을 하용하여 특정 조건에 맞는 원소를 선택하는 더 진보된 인덱싱 방법을 지원한다.\n",
    "x = torch.arange(10)\n",
    "print(x[x > 5]) # boolean mask 사용\n",
    "\n",
    "y = torch.tensor([[10, 20, 30], [40, 50, 60]])\n",
    "idx = torch.tensor([0, 2])\n",
    "print(y[0, idx]) # 행렬 y의 0번행에서 0, 2번열만 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf8e46-68c1-4088-9a94-cb7d8f1bc205",
   "metadata": {},
   "source": [
    "k_tensor_reshaping.py\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781a5ce-97d5-4512-8c25-d21ed89a6719",
   "metadata": {},
   "source": [
    "### 텐서 Reshaping\n",
    "- torch.view(input, *shape) & torch.reshape(input, shape): 텐서의 데이터를 변경하지 않고 shape을 바꾸는 함수. 반환된 텐서는 원래 텐서와 동일한 메모리(데이터)를 공유함.\n",
    "- torch.view()는 연속적인 텐서에서만 동작함.\n",
    "- torch.reshape()는 비연속적 텐서에 대해서는 복사본을 반환할 수도 있음.\n",
    "\n",
    "- torch.squeeze(): 크기가 1이거나, 크기가 1인 지정한 차원을 제거함.\n",
    "- unsqueeze와 달리 () 안에 숫자를 꼭 넣지 않아도 되지만, 이 경우 크기가 1인 모든 차원을 제거함. 반대로 shape이 (2, 3)인 텐서 등 크기가 1인 차원이 없는 텐서에 squeeze를 사용하면 변화 없음.\n",
    "- torch.flatten(input, start_dim=0, end_dim=-1): 텐서를 1차원 텐서로 펼침(flatten). start_dim과 end_dim을 지정하면 해당 구간의 차원만 펼쳐줌. 즉, shape를 start_dim×end_dim으로 만들어줌.\n",
    "- torch.permute(input, dims)&torch.transpose(input, dim0, dim1): 원래 텐서의 데이터를 복사하지 않고, 차원의 순서를 바꾼 뷰를 반환.\n",
    "- torch.permute(input, dims): 텐서의 모든 차원의 순서를 원하는 대로 재배치 가능. dims는 튜플/리스트 형태로 전체 차원의 새로운 순서를 지정.\n",
    "- torch.transpose(input, dims0, dims1): 지정한 두 개의 차원만 서로 맞바꿈. 2차원 행렬에서는 행렬의 전치와 같은 동작을 수행.\n",
    "- torch.t(): 2차원 텐서만 입력으로 받아 0번째 차원과 1번째 차원을 전치시킴. 행렬 전치 연산 전용 단축 함수로 쓰임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fb51dab-a3a7-4fec-be5f-b24239ba274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Tensor Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "087cfec9-4d31-4246-a0ef-fae54bd4a6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)  # Shape becomes (3, 2)\n",
    "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4)\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
    "print(t4)\n",
    "print(t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24e2209d-4897-47c9-bccb-3d40b94dbcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (1, 3, 1)\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "t7 = t6.squeeze()  # Shape becomes (3,)\n",
    "\n",
    "# Remove dimension at position 0\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1)\n",
    "print(t7)\n",
    "print(t8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d6ca487-b2f6-40cb-83f4-758119fcea54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (3,)\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# torch.unsqueeze(): 텐서에 새로운 차원을 지정한 위치에 추가\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1)\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3)\n",
    "print(t12, t12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea55b7ed-cc74-49db-aa67-ecc8e0b73b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (2, 3)\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "t14 = t13.flatten()  # Shape becomes (6,)\n",
    "\n",
    "print(t14)\n",
    "\n",
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15) # shape는 2×2×2=8\n",
    "\n",
    "t17 = torch.flatten(t15, start_dim=1) # shape는 2×(2×2)→2×4\n",
    "\n",
    "print(t16)\n",
    "print(t17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d929f586-21b0-4fe7-9eb9-bdb127c47f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3])\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Permute the dimensions\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a548826-12b6-4afd-b98d-e439b98c9cac",
   "metadata": {},
   "source": [
    "l_tensor_concat.py\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24adc9cb-7561-4e5b-995e-89dfe051adb9",
   "metadata": {},
   "source": [
    "### 텐서 스택킹\n",
    "- 여러 개의 텐서를 지정한 차원(dim)을 따라 연결시킨 새로운 텐서를 반환하는 기법.\n",
    "- 연결은 여러 개의 텐서의 지정된 차원'만'을 더하는 식으로 이루어짐.\n",
    "- torch.concat()은 torch.cat()의 별칭임.\n",
    "- torch.stack(tensors, dim=0): 여러 텐서를 새로운 차원으로 쌓아서(stack) 하나의 텐서로 병합한다. 병합된 텐서는 지정된 차원의 추가로 원래 텐서보다 1차원 더 많으며, 사용 시 모든 텐서의 크기가 동일해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e3402b6-a2d4-482f-b7ab-8414892ecec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d1f6c78-f8eb-4098-ac8f-5c5f65f29e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# 주의: torch.zeros는 데이터가 아닌 size가 인자로 들어감\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)\n",
    "print(t4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3322b360-a36b-4f7b-aed3-e74899287874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "print(t7.shape)  # >>> torch.Size([8])\n",
    "print(t7)  # >>> tensor([0, 1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d588edc9-c8a7-4d3f-9aa6-4d87c34231d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8, t9), dim=0)\n",
    "print(t10.size())  # >>> torch.Size([4, 3])\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())  # >>>torch.Size([2, 6])\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7216ff64-d398-499f-8434-6303c7ea22b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n"
     ]
    }
   ],
   "source": [
    "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())  # >>> torch.Size([6, 3])\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())  # >>> torch.Size([2, 9])\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17852b1f-9486-4e5a-bb48-1236b67e0efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)\n",
    "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1)\n",
    "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)\n",
    "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337ea0d-a2ac-4c2b-8e85-a352e70704ea",
   "metadata": {},
   "source": [
    "m_tensor_stacking.py\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9b4ac13-dbdb-48ef-b14e-fcff924be0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "96c643e6-3448-4afa-a84c-21cb4d4f6dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]]) # 2×3 텐서\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]]) # 2×3 텐서\n",
    "\n",
    "# 아래의 stack과 cat은 출력 결과가 같다.\n",
    "t3 = torch.stack([t1, t2], dim=0) # t1과 t2에 0번 차원을 추가 후 병합.\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "print(t3.shape, t3.equal(t4))\n",
    "\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9198303e-b599-4f42-9d4a-4489599c305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
    "\n",
    "print(t9.size(), t10.size())\n",
    "# >>> torch.Size([3]) torch.Size([3])\n",
    "\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())  # >>> torch.Size([2,3])\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "# >>> True\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())  # >>> torch.Size([3,2])\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))\n",
    "# >>> True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756a1b4-7353-4c69-ba1c-cc17120b737c",
   "metadata": {},
   "source": [
    "n_tensor_vstack_hstack.py\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b7bfccdf-6022-4042-a579-687245317535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d200f8c-8fe5-4c19-98a7-af50fa495a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.vstack(tensors): 텐서를 수직=세로 방향=행 기준으로 쌓아 병합. 모든 텐서의 '열' 수가 동일해야 함.\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "# >>> (4, 2, 3)\n",
    "\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb107005-4f6b-4d76-9bfa-b4ccdbc2a357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.hstack(tensors): 텐서를 수평으로=가로 방향=열 기준으로 쌓아 병합. 모든 텐서의 '행' 수가 동일해야 함.\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)\n",
    "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])\n",
    "\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "# >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185fe2b-2234-4072-84bf-a270b7d939ac",
   "metadata": {},
   "source": [
    "# 숙제 후기\n",
    " PyTorch 라이브러리와 Tensor 자료구조의 문법, Jupyter Notebook 플랫폼과 마크다운 문법까지 딥러닝 공부에 필요한 여러 프로그램들을 실습해볼 수 있는 유익한 과제였습니다. 오프라인 강의로 처음 배울 때와 온라인 강의로 보고듣기만 할 때는 이해가 잘 되지 않던 문법들도 차근차근 실습해보고 제가 이해한 내용을 주석 등 글로 정리하면서 훨씬 수월하게 익힐 수 있었습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
