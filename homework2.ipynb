{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd19952c-dd67-4f42-8f31-a0e0184295cf",
   "metadata": {},
   "source": [
    "### [요구사항 1]\ttitanic 딥러닝 모델 기본 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T17:07:59.976429Z",
     "start_time": "2025-10-16T17:07:49.241706Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "407eaeb4-1fa3-4a55-b703-d0909606c2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH: C:\\Users\\WJM\\git\\link_dl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "try:\n",
    "    BASE_PATH = str(Path(__file__).resolve().parent.parent) # 현재 실행 중인 파일의 상위 디렉토리 두 단계(=link_dl)을 호출\n",
    "except NameError: # Jupyter Notebook처럼 __file__이 없는 환경에서는 os.getcwd()로 현재 작업 디렉토리를 기준으로 설정\n",
    "    BASE_PATH = str(Path(os.getcwd()).resolve().parent)\n",
    "print(\"BASE_PATH:\", BASE_PATH)\n",
    "\n",
    "# 기본 디렉토리를 Python Path에 추가 => Python 인터프리터가 Python 라이브러리와 애플리케이션을 찾을 위치를 안내\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "# _03_homeworks.homework_2 디렉토리의 titanic_dataset.py의 get_preprocessed_dataset 함수를 import\n",
    "from _03_homeworks.homework_2.titanic_dataset \\\n",
    "    import get_preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe4d501-ccbe-4186-a22a-8b9612b44553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset() # titanic_dataset.py에서 전처리된 훈련, 검증, 평가용 데이터셋 호출\n",
    "    print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "    # 학습, 검증, 평가용 데이터로더로 변환\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=wandb.config.batch_size, shuffle=True) # wandb에서 설정한 배치 크기로 설정, 매 Epoch마다 데이터 순서를 섞어 학습\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset)) # 모든 검증용 데이터를 한꺼번에 번에 배치 처리\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset)) # 모든 평가용 데이터를 한꺼번에 배치 처리\n",
    "\n",
    "    return train_data_loader, validation_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21fb017-e435-4362-9cc4-c916f9a63d50",
   "metadata": {},
   "source": [
    "### 참고: Dataset과 Dataloader의 차이\n",
    "1. 데이터셋(Dataset): 테이블, 배열, CSV, JSON 등으로 구성된 원본 데이터의 모음\n",
    "2. 데이터로더(Dataloader): Pytorch에서 제공하는 데이터 적재 유틸리티. 데이터셋을 사용자가 설정한 배치 크기 단위로 나누어 배치들로 만들고, training loop에서 배치 단위로 사용할 수 있는 반복 가능한 객체(iterator)로 만들어준다. Dataloader로 만들지 않으면 데이터셋의 데이터는 training loop에서 샘플 단위로 하나씩만 반복할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b203c963-267e-401b-a463-2374a7523423",
   "metadata": {},
   "source": [
    "### [요구사항 2] Activation Function과 Batch Size 변경 및 선택하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35cf40b0-8e9d-4a4c-8b7d-bf68308df947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 모델\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output): # titanic 데이터의 피처 개수=10, 출력 클래스(0(사망), 1(생존))의 개수=2\n",
    "        super().__init__()\n",
    "\n",
    "        activation_name = wandb.config.activation_fn\n",
    "        if activation_name == \"ReLU\":\n",
    "            activation_fn = nn.ReLU()\n",
    "        elif activation_name == \"Sigmoid\":\n",
    "            activation_fn = nn.Sigmoid()\n",
    "        elif activation_name == \"ELU\":\n",
    "            activation_fn = nn.ELU()\n",
    "        elif activation_name == \"LeakyReLU\":\n",
    "            activation_fn = nn.LeakyReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation_name}\")\n",
    "\n",
    "        self.model = nn.Sequential( # 딥러닝 은닉층 구성. 층의 개수는 wandb에서 설정.\n",
    "            nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]), # 첫번째 은닉층. 유닛 개수는 64개\n",
    "            activation_fn, # 활성화 함수\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]), # 두번째 은닉층. 유닛 개수는 32개\n",
    "            activation_fn,\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[1], n_output), # 출력층\n",
    "        )\n",
    "\n",
    "    # 순전파: 입력값 x를 self.model로 통과시켜 출력을 생성, 즉 다음 은닉층으로 넘어가게 함.\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca707cf-7a44-409b-b800-67c9942a3884",
   "metadata": {},
   "source": [
    "![Activation Function과 Batch Size별 Training loss와 Validation loss 비교](https://i.imgur.com/abMQe9e.jpeg)\n",
    "\n",
    "활성화 함수로 ReLU를 선택하고 배치 크기를 64로 설정했을 때가 Validation loss가 가장 작게 나오고, Training loss 역시 아주 높지는 않기에 최고의 조합으로 선정하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7ae959e-9e55-4ebf-b461-c2ac40a7cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader, device): # 학습할 모델, 모델의 패러미터 업데이트용 optimizer, 학습용과 검증용 Dataloader, 학습 장치를 매개변수로 받음.\n",
    "    n_epochs = wandb.config.epochs # wandb에서 설정한 Epoch 수=200 => 모든 데이터를 200번 보겠음.\n",
    "    loss_fn = nn.BCEWithLogitsLoss() # 이진 분류용(0 또는 1=사망 혹은 생존) 손실 함수\n",
    "    best_validation_loss = float('inf') # 가장 검증 손실이 작게 나온 모델을\n",
    "    best_path = os.path.join(BASE_PATH, \"best_model.pth\") # 최고의 모델로 보아 다음 경로에 저장함.\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1): # 1부터 200까지 한 Epoch마다 모델 훈련\n",
    "        model.train() # 모델 훈련\n",
    "        loss_train = 0.0\n",
    "        num_trains = 0\n",
    "        for train_batch in train_data_loader: # 훈련용 Dataloader에서 배치 단위=데이터 64개씩 반복\n",
    "            inputs = train_batch['input'].to(device) # 입력값과\n",
    "            targets = train_batch['target'].to(device).float().unsqueeze(1) # 타겟값을 device로 이동시켜서\n",
    "            output_train = model(inputs) # 입력값에 따른 모델의 예측값 계산\n",
    "            loss = loss_fn(output_train, targets) # 손실, 즉 타겟값과 모델의 예측값의 차이 계산\n",
    "\n",
    "            optimizer.zero_grad() # 이전 기울기 초기화\n",
    "            loss.backward() # 역전파 알고리즘으로 기울기 계산\n",
    "            optimizer.step() # 패러미터 업데이트\n",
    "\n",
    "            batch_size = inputs.size(0)\n",
    "            loss_train += loss.item() * batch_size # 배치별 손실 평균값 × 배치 크기 = 배치의 총 손실\n",
    "            num_trains += batch_size # 지금까지 학습한 샘플의 수\n",
    "        avg_train = loss_train / num_trains if num_trains > 0 else float('nan') # 전체 학습 데이터 기준 평균 손실\n",
    "\n",
    "        model.eval() # 모델 검증\n",
    "        loss_validation = 0.0\n",
    "        num_validations = 0\n",
    "        with torch.no_grad(): # 기울기 계산 종료\n",
    "            for validation_batch in validation_data_loader: # 검증용 데이터들로 모델 검증\n",
    "                inputs = validation_batch['input'].to(device)\n",
    "                targets = validation_batch['target'].to(device).float().unsqueeze(1)\n",
    "                output_validation = model(inputs)\n",
    "                loss = loss_fn(output_validation, targets)\n",
    "                batch_size = inputs.size(0)\n",
    "                loss_validation += loss.item() * batch_size\n",
    "                num_validations += batch_size\n",
    "        avg_validation = loss_validation / num_validations if num_validations > 0 else float('nan')\n",
    "\n",
    "        wandb.log({ # wandb에 전체 학습/검증용 데이터별 평균 손실 기록\n",
    "            \"Epoch\": epoch,\n",
    "            \"Training loss\": avg_train,\n",
    "            \"Validation loss\": avg_validation\n",
    "        })\n",
    "\n",
    "        if avg_validation < best_validation_loss: # 현재까지 최고(로 낮은) 검증 손실보다 더 낮은 검증 손실을 보이는 모델이 있다면\n",
    "            best_validation_loss = avg_validation # 최고 검증 손실값을 갱신하고\n",
    "            torch.save({ # 새로운 최고 성능의 모델과 그 저장 경로를 갱신하여 저장\n",
    "                \"Epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"Validation loss\": avg_validation\n",
    "            }, best_path)\n",
    "            print(\n",
    "                f\"Epoch {epoch},\"\n",
    "                f\"Training loss {avg_train:.4f}, \"\n",
    "                f\"Validation loss {avg_validation:.4f}\"\n",
    "                f\"Saved best model to {best_path}\"\n",
    "            )\n",
    "\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0667692-3d4c-4117-9508-a1dc1363c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop 중에 나온 최고 성능 모델을 복원\n",
    "def make_submission(checkpoint_path, model_constructor, n_input, n_output, test_data_loader, device, submission_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device) # 전체 학습 모델 중 가장 작은 Validation loss를 기록한 모델 호출\n",
    "    model = model_constructor(n_input, n_output) # 새로운 모델 객체를 생성하여\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"]) # 최고 성능 모델의 패러미터를 로드\n",
    "    model.to(device) # device=학습 장치로 이동 후\n",
    "    model.eval() # 평가\n",
    "\n",
    "    rows = []\n",
    "    with torch.no_grad(): # 평가 시 기울기 계산 비활성화\n",
    "        for test_batch in test_data_loader:\n",
    "            inputs = test_batch['input'].to(device) # 평가용 Dataloader의 입력값들을\n",
    "            outputs = model(inputs) # 모델에 입력하여 출력값 계산\n",
    "            if outputs.dim() == 2 and outputs.size(1) == 1:\n",
    "                outputs = outputs.squeeze()\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            preds = (probs >= 0.5).astype(int) # 출력값이 50% 이상이면 1(생존), 미만이면 0(사망)으로 예측\n",
    "\n",
    "            # 평가용 데이터셋의 승객별 PassengerId를 가져와 생존/사망 여부를 기록. 만약 PassengerId가 없다면 인덱스로 Id 생성\n",
    "            ids = test_batch.get('PassengerId', list(range(len(rows), len(rows)+len(preds))))\n",
    "            for pid, p in zip(ids, preds):\n",
    "                rows.append({\"PassengerId\": int(pid), \"Survived\": int(p)})\n",
    "\n",
    "     # 예측 결과를 pandas DataFrame으로 변환\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.astype({\"PassengerId\": int, \"Survived\": int})\n",
    "    df = df[[\"PassengerId\", \"Survived\"]]\n",
    "    df.to_csv(submission_path, index=False) # csv로 저장\n",
    "    print(f\"Saved submission to {submission_path} ({len(df)} rows)\") # 저장 경로와 행 개수 출력\n",
    "    return submission_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef758252-cdbd-45b6-b6e6-96a4685cc8c3",
   "metadata": {},
   "source": [
    "### ※ 캐글에 제출할 submission은 PassengerId와 Survived(0 혹은 1) 형태의 테이블이어야 하기 때문에 titanic_dataset.py를 일부 수정하여 평가용 데이터셋에 PassengerId 삽입\n",
    "\n",
    "class TitanicTestDataset(Dataset):\n",
    "  def __init__(self, X, passenger_ids): # ', passenger_ids' 추가\n",
    "    self.X = torch.FloatTensor(X)\n",
    "    self.passenger_ids = passenger_ids # 추가\n",
    "    \n",
    "  ...\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    feature = self.X[idx]\n",
    "    return {'input': feature, 'PassengerId': self.passenger_ids[idx]} # ''PassengerId': self.passenger_ids[idx]' 추가\n",
    "\n",
    "def get_preprocessed_dataset():\n",
    "    ...\n",
    "    \n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
    "    test_ids = test_df[\"PassengerId\"].values # 원본 PassengerId를 불러오기 위해 추가한 코드\n",
    "    test_dataset = TitanicTestDataset(test_X.values, passenger_ids=test_ids) # ', passenger_ids=test_ids' 추가\n",
    "    #print(test_dataset)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2616f8e-478d-4ddb-9980-ea53734a7bb2",
   "metadata": {},
   "source": [
    "### [요구사항 3]\t테스트 및 submission.csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285694ed-6549-41e8-82dc-7edb0aabadfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    # config 딕셔너리 준비 후\n",
    "    config = {\n",
    "        'epochs': args.epochs,\n",
    "        'batch_size': args.batch_size,\n",
    "        'learning_rate': args.learning_rate,\n",
    "        'n_hidden_unit_list': args.n_hidden_unit_list,\n",
    "        'activation_fn': args.activation_fn\n",
    "    }\n",
    "\n",
    "    # wandb 실행\n",
    "    wandb.init(\n",
    "        mode=\"online\" if args.wandb else \"disabled\",\n",
    "        project=\"my_model_training\",\n",
    "        notes=\"Deep Learning and Exercise Homework2\",\n",
    "        tags=[\"my_model\", \"titanic\"],\n",
    "        name=current_time_str,\n",
    "        config=config\n",
    "    )\n",
    "    print(args)\n",
    "    print(wandb.config)\n",
    "\n",
    "    # 훈련, 검증, 평가용 Dataloader 생성\n",
    "    train_data_loader, validation_data_loader, test_data_loader = get_data()\n",
    "\n",
    "    # device 결정\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(\"#\" * 50, 1)\n",
    "\n",
    "    # 첫번째 배치로부터 입력값의 수를 계산\n",
    "    first_batch = next(iter(train_data_loader))\n",
    "    input_shape = first_batch['input'].shape\n",
    "    n_input = input_shape[1]\n",
    "    n_output = 1\n",
    "\n",
    "    # MyModel 생성\n",
    "    model = MyModel(n_input, n_output).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    # MyModel의 training loop를 시행하여 최고 성능 모델 및 그 경로 반환\n",
    "    best_path = training_loop(model, optimizer, train_data_loader, validation_data_loader, device)\n",
    "\n",
    "    # 최고 성능 모델로 submission.csv 생성\n",
    "    make_submission(best_path, lambda n_in, n_out: MyModel(n_in, n_out),\n",
    "                    n_input, n_output, test_data_loader, device, os.path.join(BASE_PATH, \"submission.csv\"))\n",
    "    \n",
    "    wandb.finish() # wandb 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff79c20-03b1-4da9-accf-c79b95096f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\WJM\\git\\link_dl\\_04_your_code\\wandb\\run-20251017_190224-a6cq1z2q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wjm0423-korea-university-of-technology-and-education/my_model_training/runs/a6cq1z2q' target=\"_blank\">2025-10-17_19-02-24</a></strong> to <a href='https://wandb.ai/wjm0423-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wjm0423-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/wjm0423-korea-university-of-technology-and-education/my_model_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wjm0423-korea-university-of-technology-and-education/my_model_training/runs/a6cq1z2q' target=\"_blank\">https://wandb.ai/wjm0423-korea-university-of-technology-and-education/my_model_training/runs/a6cq1z2q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(wandb=True, batch_size=64, epochs=200, learning_rate=0.001, n_hidden_unit_list=[32, 16], activation_fn='ReLU')\n",
      "{'epochs': 200, 'batch_size': 64, 'learning_rate': 0.001, 'n_hidden_unit_list': [32, 16], 'activation_fn': 'ReLU'}\n",
      "Index(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n",
      "       'Embarked', 'title', 'family_num', 'alone'],\n",
      "      dtype='object')\n",
      "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  title  \\\n",
      "0       0.0       3    1  22.0      1      0   7.2500         2      2   \n",
      "1       1.0       1    0  38.0      1      0  71.2833         0      3   \n",
      "2       1.0       3    0  26.0      0      0   7.9250         2      1   \n",
      "3       1.0       1    0  35.0      1      0  53.1000         2      3   \n",
      "4       0.0       3    1  35.0      0      0   8.0500         2      2   \n",
      "5       0.0       3    1  29.0      0      0   8.4583         1      2   \n",
      "6       0.0       1    1  54.0      0      0  51.8625         2      2   \n",
      "7       0.0       3    1   2.0      3      1  21.0750         2      0   \n",
      "8       1.0       3    0  27.0      0      2  11.1333         2      3   \n",
      "9       1.0       2    0  14.0      1      0  30.0708         0      3   \n",
      "\n",
      "   family_num  alone  \n",
      "0           1    0.0  \n",
      "1           1    0.0  \n",
      "2           0    1.0  \n",
      "3           1    0.0  \n",
      "4           0    1.0  \n",
      "5           0    1.0  \n",
      "6           0    1.0  \n",
      "7           4    0.0  \n",
      "8           2    0.0  \n",
      "9           1    0.0  \n",
      "Data Size: 891, Input Shape: torch.Size([891, 10]), Target Shape: torch.Size([891])\n",
      "713 178 418\n",
      "################################################## 1\n",
      "Epoch 1,Training loss 1.0752, Validation loss 0.8571Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 2,Training loss 0.8120, Validation loss 0.7206Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 3,Training loss 0.7223, Validation loss 0.6848Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 4,Training loss 0.6958, Validation loss 0.6737Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 5,Training loss 0.6874, Validation loss 0.6701Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 6,Training loss 0.6827, Validation loss 0.6655Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 7,Training loss 0.6782, Validation loss 0.6609Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 8,Training loss 0.6742, Validation loss 0.6583Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 9,Training loss 0.6708, Validation loss 0.6543Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 10,Training loss 0.6682, Validation loss 0.6519Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 11,Training loss 0.6663, Validation loss 0.6496Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 12,Training loss 0.6638, Validation loss 0.6476Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 14,Training loss 0.6612, Validation loss 0.6451Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 15,Training loss 0.6595, Validation loss 0.6440Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 16,Training loss 0.6577, Validation loss 0.6423Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 18,Training loss 0.6558, Validation loss 0.6415Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 20,Training loss 0.6537, Validation loss 0.6395Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 22,Training loss 0.6518, Validation loss 0.6364Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 23,Training loss 0.6501, Validation loss 0.6355Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 24,Training loss 0.6489, Validation loss 0.6346Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 25,Training loss 0.6476, Validation loss 0.6342Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 26,Training loss 0.6479, Validation loss 0.6330Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 27,Training loss 0.6464, Validation loss 0.6319Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 30,Training loss 0.6444, Validation loss 0.6300Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 31,Training loss 0.6433, Validation loss 0.6293Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 32,Training loss 0.6435, Validation loss 0.6287Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 33,Training loss 0.6419, Validation loss 0.6280Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 35,Training loss 0.6406, Validation loss 0.6270Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 37,Training loss 0.6387, Validation loss 0.6266Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 38,Training loss 0.6383, Validation loss 0.6256Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 39,Training loss 0.6370, Validation loss 0.6248Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 41,Training loss 0.6378, Validation loss 0.6243Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 44,Training loss 0.6363, Validation loss 0.6234Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 46,Training loss 0.6338, Validation loss 0.6223Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 47,Training loss 0.6331, Validation loss 0.6212Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 48,Training loss 0.6328, Validation loss 0.6208Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 49,Training loss 0.6327, Validation loss 0.6200Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 50,Training loss 0.6314, Validation loss 0.6198Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 52,Training loss 0.6313, Validation loss 0.6191Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 53,Training loss 0.6310, Validation loss 0.6190Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 54,Training loss 0.6302, Validation loss 0.6187Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 55,Training loss 0.6302, Validation loss 0.6184Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 56,Training loss 0.6290, Validation loss 0.6181Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 58,Training loss 0.6287, Validation loss 0.6175Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 60,Training loss 0.6290, Validation loss 0.6175Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 61,Training loss 0.6274, Validation loss 0.6169Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 62,Training loss 0.6276, Validation loss 0.6167Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 63,Training loss 0.6279, Validation loss 0.6160Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 66,Training loss 0.6255, Validation loss 0.6158Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 69,Training loss 0.6243, Validation loss 0.6147Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 70,Training loss 0.6244, Validation loss 0.6143Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 72,Training loss 0.6250, Validation loss 0.6142Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 73,Training loss 0.6240, Validation loss 0.6136Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 74,Training loss 0.6224, Validation loss 0.6136Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 75,Training loss 0.6232, Validation loss 0.6134Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 76,Training loss 0.6219, Validation loss 0.6131Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 78,Training loss 0.6243, Validation loss 0.6126Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 80,Training loss 0.6211, Validation loss 0.6121Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 83,Training loss 0.6202, Validation loss 0.6113Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 84,Training loss 0.6194, Validation loss 0.6111Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 85,Training loss 0.6196, Validation loss 0.6104Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 86,Training loss 0.6191, Validation loss 0.6104Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 87,Training loss 0.6211, Validation loss 0.6101Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 88,Training loss 0.6187, Validation loss 0.6092Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 89,Training loss 0.6180, Validation loss 0.6092Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 94,Training loss 0.6166, Validation loss 0.6089Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 97,Training loss 0.6158, Validation loss 0.6082Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 99,Training loss 0.6151, Validation loss 0.6073Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 100,Training loss 0.6143, Validation loss 0.6069Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 101,Training loss 0.6147, Validation loss 0.6067Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 102,Training loss 0.6141, Validation loss 0.6066Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 104,Training loss 0.6129, Validation loss 0.6062Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 106,Training loss 0.6130, Validation loss 0.6057Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 108,Training loss 0.6124, Validation loss 0.6053Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 110,Training loss 0.6117, Validation loss 0.6048Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 117,Training loss 0.6094, Validation loss 0.6045Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 119,Training loss 0.6104, Validation loss 0.6044Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 120,Training loss 0.6085, Validation loss 0.6040Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 121,Training loss 0.6081, Validation loss 0.6035Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 123,Training loss 0.6095, Validation loss 0.6028Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 125,Training loss 0.6075, Validation loss 0.6027Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 126,Training loss 0.6079, Validation loss 0.6018Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 130,Training loss 0.6069, Validation loss 0.6016Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 136,Training loss 0.6057, Validation loss 0.6011Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 137,Training loss 0.6043, Validation loss 0.6009Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 140,Training loss 0.6039, Validation loss 0.6006Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 141,Training loss 0.6035, Validation loss 0.6003Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 142,Training loss 0.6036, Validation loss 0.5998Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 143,Training loss 0.6037, Validation loss 0.5995Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 145,Training loss 0.6031, Validation loss 0.5993Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 146,Training loss 0.6034, Validation loss 0.5990Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 150,Training loss 0.6031, Validation loss 0.5987Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 151,Training loss 0.6013, Validation loss 0.5984Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 155,Training loss 0.6014, Validation loss 0.5976Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 157,Training loss 0.6002, Validation loss 0.5974Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 158,Training loss 0.6001, Validation loss 0.5972Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 159,Training loss 0.5995, Validation loss 0.5971Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 161,Training loss 0.5998, Validation loss 0.5970Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 165,Training loss 0.5995, Validation loss 0.5965Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 169,Training loss 0.5990, Validation loss 0.5952Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 172,Training loss 0.5974, Validation loss 0.5950Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 176,Training loss 0.5969, Validation loss 0.5947Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 177,Training loss 0.5975, Validation loss 0.5944Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 178,Training loss 0.5966, Validation loss 0.5944Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 181,Training loss 0.5954, Validation loss 0.5942Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 185,Training loss 0.5954, Validation loss 0.5940Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 186,Training loss 0.5959, Validation loss 0.5935Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 187,Training loss 0.5948, Validation loss 0.5929Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 189,Training loss 0.5944, Validation loss 0.5927Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 198,Training loss 0.5928, Validation loss 0.5925Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Epoch 200,Training loss 0.5932, Validation loss 0.5916Saved best model to C:\\Users\\WJM\\git\\link_dl\\best_model.pth\n",
      "Saved submission to C:\\Users\\WJM\\git\\link_dl\\submission.csv (418 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>Training loss</td><td>█▇▆▆▅▅▅▄▄▄▄▄▄▃▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation loss</td><td>█▇▆▆▆▅▄▄▄▄▄▄▃▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>200</td></tr><tr><td>Training loss</td><td>0.59318</td></tr><tr><td>Validation loss</td><td>0.59156</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2025-10-17_19-02-24</strong> at: <a href='https://wandb.ai/wjm0423-korea-university-of-technology-and-education/my_model_training/runs/a6cq1z2q' target=\"_blank\">https://wandb.ai/wjm0423-korea-university-of-technology-and-education/my_model_training/runs/a6cq1z2q</a><br> View project at: <a href='https://wandb.ai/wjm0423-korea-university-of-technology-and-education/my_model_training' target=\"_blank\">https://wandb.ai/wjm0423-korea-university-of-technology-and-education/my_model_training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251017_190224-a6cq1z2q\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://docs.wandb.ai/guides/track/config\n",
    "if __name__ == \"__main__\":\n",
    "    # Argparse를 사용하여 명령어에 \"--옵션\"을 넣을 때만 해당 옵션을 실행하고, 아니면 각 옵션별 default를 실행\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"Use wandb or not\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-b\", \"--batch_size\", type=int, default=64, help=\"Batch size\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-e\", \"--epochs\", type=int, default=200, help=\"Epochs\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\", type=float, default=1e-3, help=\"Learning rate\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--n_hidden_unit_list\", nargs=\"+\", type=int, default=[32, 16],\n",
    "        help=\"Hidden units list, e.g. --n_hidden_unit_list 32 16\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--activation_fn\", type=str, default=\"ReLU\", choices=[\"Sigmoid\", \"ReLU\", \"ELU\", \"LeakyReLU\"],\n",
    "        help=\"Actiavation function to use in the model\"\n",
    "    )\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    from types import SimpleNamespace\n",
    "    args = SimpleNamespace(\n",
    "        wandb = True,\n",
    "        batch_size = 64,\n",
    "        epochs = 200,\n",
    "        learning_rate = 1e-3,\n",
    "        n_hidden_unit_list = [32, 16],\n",
    "        activation_fn = \"ReLU\"\n",
    "    )\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f290f-1756-4b75-9f53-0e18cfa43269",
   "metadata": {},
   "source": [
    "### [요구사항 4]\tsubmission.csv제출 및 등수확인\n",
    "![캐글 점수 및 순위](https://i.imgur.com/spqbUaG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cfa5b7-15ba-4896-a9d2-23520ce79e50",
   "metadata": {},
   "source": [
    "# 숙제 후기\n",
    "wandb, Argparse 등 처음 보는 라이브러리들과 Epoch와 Batch별 훈련을 거듭할 때마다 갱신되는 Training loss와 Validation loss를 통한 최고 성능 모델 업데이트, 활성화 함수와 배치 크기별 모델 성능 비교 등 다양한 실습을 경험해볼 수 있어 매우 유익한 과제였습니다. 실제로 손실 함수값을 기준으로 여러가지 요소를 변경해가며 최고 성능을 내는 모델을 찾아가는 과정도 재미있었습니다.\n",
    "한가지 아쉬웠던 점은 평가용 데이터셋까지 PassageId 피처가 제거돼 있어서 따로 titanic_dataset.py를 수정해야 했던 것이었습니다. 최대한 titanic_dataset.py는 수정하지 않고 과제를 진행하고 싶어서 샘플마다 무작위로 PassageId를 부여하는 등 해결책을 찾느라 시간을 많이 소비했는데, 캐글에 submission 제출 시 원본 PassageId가 반드시 필요하므로 처음부터 남겨져 있더라면 더 좋았을 것 같습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
